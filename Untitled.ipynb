{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310f2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package_name\n",
      "  Downloading package_name-0.1.tar.gz (782 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: package_name\n",
      "  Building wheel for package_name (setup.py): started\n",
      "  Building wheel for package_name (setup.py): finished with status 'done'\n",
      "  Created wheel for package_name: filename=package_name-0.1-py3-none-any.whl size=1232 sha256=f2dbfd4112be66501320f8e80e936ca4a2b22a92e42497046e323e48b478b4e3\n",
      "  Stored in directory: c:\\users\\sili7485\\appdata\\local\\pip\\cache\\wheels\\c3\\04\\a0\\8bb097c240101d62865b23bd6efb62c8747b3d9e5666769764\n",
      "Successfully built package_name\n",
      "Installing collected packages: package_name\n",
      "Successfully installed package_name-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c977a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f7539d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2119417417.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\sili7485\\AppData\\Local\\Temp\\ipykernel_21508\\2119417417.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install -U pip setuptools wheel\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1518fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e577f620",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21508\\2908642691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# spacy for lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Plotting tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import singledispatch\n",
    "from collections import Counter\n",
    "\n",
    "# singledispatchmethod in functools from python3.8+\n",
    "try:\n",
    "    from functools import singledispatchmethod\n",
    "except ImportError:\n",
    "    from singledispatchmethod import singledispatchmethod\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords') #only on first install\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','please','au', 'hi','hello','help',\n",
    "                  'find','cant','one','still','ask','st','yes','im','dont','none','cannot',\n",
    "                  'want','know','see','whats','could','back','would_like','wondering','able',\n",
    "                  'hey','need','get','take','like','copy','nan','writing','ok','appreciate',\n",
    "                  'okay','thanks','make','trying','another','havent','would','thank','yet','receive',\n",
    "                  'looking','someone', 'contact_us', 'popular_questions','already','idont',\n",
    "                  'ive','wanted','today','first','information','talk','person','long','work',\n",
    "                  'week','regarding','questions','two','pm','sent','got','says','whether','youll',\n",
    "                  'said', 'hi', 'dear', 'hello'])\n",
    "stop_words.extend(['question', 'related', 'detail','found','otherwise','sorry','understand','try','rephrase','try','go','either',\n",
    "                  'however','includes','wish','also','sure','may','must','including','would'])\n",
    "stop_words.extend(['regard', 'give', 'let'])\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punct = list(string.punctuation.replace(\"'\", 'â€™'))\n",
    "        self.token_list = [\n",
    "          '|', '------','____','If you have received this email in error ','Any use, distribution, disclosure, or copying of this email','cheers', 'regards', 'best regards', 'kind regards',\n",
    "          'sent from:', 'from:', 'sent:', 'to:', 'cc:', 'subject:', 'Sender:','date: ',' outlook for', '<https://protect','This short survey will take','how satisfied or dissatisfied are you with our',\n",
    "          'CISCO', '[image]', '.pdf', '[cid:', 'Giving today. Changing tomorrow.','If you have a different query, you will need to submit a new enquiry', \n",
    "          'See the impact<', 'This email plus any attachments to it are confidential. Any unauthori','Read through your handbook<http','find out what you will need to enrol in your new course<https',\n",
    "          'We acknowledge and respect the', 'Please think of our environment','If you are not satisfied with this response, you can re-open the enquiry',\n",
    "          'This e-mail may contain information which is confidential', 'Before commencing your research you will need to enrol, to review the details of your approved',\n",
    "          'This email (including any attachments) may contain confidential','We acknowledge the traditional custodians of the land','This communication may contain information that is proprietary',\n",
    "          'Views expressed in this email are those of the individual sender, and are not necessarily the views of', 'You can find out how to suspend your studies<https',\n",
    "          'This email plus any attachments to it are confidential','for any other general enquiries you are welcome to contact the Student Centre',\n",
    "          '=========', 'Please quote your full name and student number in all correspondence with the University.','Your student card is valid for the duration of your enrolment',\n",
    "          'Please use your student email account if you are a current student for all communication with the University.','Thinking about taking a break from your studies',\n",
    "          'The majority of correspondence issued by the University of Sydney is sent via email. In order to ensure','You can find more information about enrolment<',\n",
    "          'To update this case please click on the link in the email below, or alternatively reply above this lin','After you have successfully enrolled, we will send a confirmation summary to your University student',\n",
    "          'If you receive this email in error', 'It is strictly forbidden to share any part of this message','If you have any questions, call us on 1800 SYD UNI'\n",
    "        ]\n",
    "        self.token_list = [t.lower() for t in self.token_list]\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if isinstance(args[0], list):\n",
    "            [self(arg) for arg in args[0]]\n",
    "        elif isinstance(args[0], pd.DataFrame):\n",
    "            self.replace_description(args[0])\n",
    "            self.remove_blacklisted_sentences(args[0])\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "    def remove_punctuations(self, text):\n",
    "        for punctuation in self.punct:\n",
    "            text = text.replace(punctuation, ' ')\n",
    "        return text\n",
    "\n",
    "    def str_blacklist(self, string):\n",
    "        str_list = string.split('\\n')\n",
    "        return '\\n'.join([s for s in str_list if not any(t in s.lower() for t in self.token_list) and len(s) != 0])\n",
    "      \n",
    "    def remove_blacklisted_sentences(self, df):\n",
    "        df['description'] = df['description'].apply(self.str_blacklist)\n",
    "\n",
    "    def replace_description(self, df):\n",
    "        df['description'] = df['description'].apply(lambda x: x.lower())\n",
    "        df['description'] = df['description'].apply(lambda x: x.replace('\\n', ' '))\n",
    "        df['description'] = df['description'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "        df['description'] = df['description'].apply(lambda x: x.replace('&gt;', ''))\n",
    "        df['description'] = df['description'].apply(self.remove_punctuations)\n",
    "        df['description'] = df['description'].apply(lambda x: str(x).replace(\" s \", \" \"))\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\"\\r\", \" \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(' university of sydney',' usyd ', case=False)\n",
    "        df['description'] = df['description'].str.replace(' university ', ' usyd ', case=False)\n",
    "        df['description'] = df['description'].str.replace(' uni ',' usyd ',case=False)  #do not use this line it will mess up unit as usydt\n",
    "        df['description'] = df['description'].str.replace(\"sydney student\",\"sydney_student\",case=False)\n",
    "        df['description'] = df['description'].str.replace(\"contact us\", \"contact_us\", case=False)\n",
    "        df['description'] = df['description'].str.replace(\"popular questions\",\"popular_questions\", case=False)\n",
    "        df['description'] = df['description'].str.replace(' center ',' centre ',case=False)\n",
    "        df['description'] = df['description'].str.replace(\"student centre\", \"student_centre\", case=False)\n",
    "        df['description'] = df['description'].str.replace(\"unit of study\",\"unit_of_study\", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" units of study \",\" unit_of_study \",case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" masters \",\" master \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\"online enquiry\", \"online_enquiry\", case=False)\n",
    "        df['description'] = df['description'].str.replace(\"online inquiry\", \"online_inquiry\", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" re enrol \",\" re_enrol \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" enroll \", \" enrol \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" enrolment \",\" enrol \",case=False)\n",
    "        df['description'] = df['description'].str.replace(\" enroling \",\" enrol \",case=False)\n",
    "        df['description'] = df['description'].str.replace(\" enroled \",\" enrol \",case=False)\n",
    "        df['description'] = df['description'].str.replace(\" enrolled \",\" enrol \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" re enrollment \", \" re_enrol \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" student id \" , \" student_id \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" log into \",\" login \",case=False)\n",
    "        df['description'] = df['description'].str.replace(\" log in \",\" login \",case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\"error message \", \" error_message \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" international students \", \" international_students \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" international student \", \" international_students \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" first year \", \" first_year \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" final year \", \" final_year \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" census date \", \" census_date \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" none of the above \", \" none_of_the_above \", case=False)\n",
    "        df['description'] = df['description'].str.replace(' units ',' unit ', case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" SID \", \" student_id \", case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" summer school \",\" summer_school \",case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" collecting \",\" collection \",case=False)\n",
    "        df['description'] = df['description'].str.replace(\" collect \",\" collection \",case=False)\n",
    "\n",
    "        df['description'] = df['description'].str.replace(\" studies \",\" study \",case=False)\n",
    "        #new\n",
    "        df['description'] = df['description'].str.replace(\" students \", \" student \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" covid\", \" coronavirus \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" corona virus \", \" coronavirus \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" suspension \", \" suspend \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" universitys \", \" usyd \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" begin \", \" start \", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" face to face \", \" face_to_face\", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" reenrol\", \" re_enrol\", case=False)\n",
    "        df['description'] = df['description'].str.replace(\" australian \", \" australia \", case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, nlp, stop_words):\n",
    "        self.stop_words = set(stop_words)\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, df, key='description', **kwargs):\n",
    "        self.data_lemmatized(df, key)\n",
    "        \n",
    "    def data_words(self, series):\n",
    "        return list(self.sent_to_words(series.values.tolist()))\n",
    "    \n",
    "    def sent_to_words(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "    def make_bigrams(self, bigram_mod, texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "    \n",
    "    def remove_stopwords(self, texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in self.stop_words] for doc in texts]\n",
    "    \n",
    "    def lemmatization(self, texts, allowed_postags=('NOUN', 'VERB')):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = self.nlp(\" \".join(sent))\n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "    \n",
    "    @singledispatchmethod\n",
    "    def data_lemmatized(self, series, key='description', **kwargs):\n",
    "        data_words = self.data_words(series)\n",
    "        data_words_nostops = self.remove_stopwords(data_words)\n",
    "        \n",
    "        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        \n",
    "        data_words_bigrams = self.make_bigrams(bigram_mod, data_words_nostops)\n",
    "        \n",
    "        return self.lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB'])\n",
    "    \n",
    "    @data_lemmatized.register(list)\n",
    "    def _(self, df_list, key='description', **kwargs):\n",
    "        return [self.data_lemmatized(df, key, **kwargs) for df in df_list]\n",
    "    \n",
    "    @data_lemmatized.register(pd.DataFrame)\n",
    "    def _(self, df, key='description', out_key='data_words', **kwargs):\n",
    "        df[out_key] = self.data_lemmatized(df[key], key='description', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82c756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bf731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02448000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "corpus_constructor = Corpus(nlp, stop_words)\n",
    "\n",
    "preprocessor(df)\n",
    "corpus_constructor(df)\n",
    "\n",
    "topic_constructor = TopicConstructor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97551601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c6e179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b42ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44b4a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46311532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f51bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f659d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e589586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a420b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f852bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacf00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f048bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c569b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc297202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c4de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c7e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12907be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e024dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a7598b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4839266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
