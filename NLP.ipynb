{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b0fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551affde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:164: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:281: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:865: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1121: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1149: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1379: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1621: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_least_angle.py:1755: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_lda.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import singledispatch\n",
    "from collections import Counter\n",
    "\n",
    "# singledispatchmethod in functools from python3.8+\n",
    "try:\n",
    "    from functools import singledispatchmethod\n",
    "except ImportError:\n",
    "    from singledispatchmethod import singledispatchmethod\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords') #only on first install\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100d80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','please','au', 'hi','hello','help',\n",
    "                  'find','cant','one','still','ask','st','yes','im','dont','none','cannot',\n",
    "                  'want','know','see','whats','could','back','would_like','wondering','able',\n",
    "                  'hey','need','get','take','like','copy','nan','writing','ok','appreciate',\n",
    "                  'okay','thanks','make','trying','another','havent','would','thank','yet','receive',\n",
    "                  'looking','someone', 'contact_us', 'popular_questions','already','idont',\n",
    "                  'ive','wanted','today','first','information','talk','person','long','work',\n",
    "                  'week','regarding','questions','two','pm','sent','got','says','whether','youll',\n",
    "                  'said', 'hi', 'dear', 'hello'])\n",
    "stop_words.extend(['question', 'related', 'detail','found','otherwise','sorry','understand','try','rephrase','try','go','either',\n",
    "                  'however','includes','wish','also','sure','may','must','including','would'])\n",
    "stop_words.extend(['regard', 'give', 'let'])\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe30db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WR_ID</th>\n",
       "      <th>PROB_TYPE</th>\n",
       "      <th>REQUESTOR</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>DATE_REQUESTED</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>BL_ID</th>\n",
       "      <th>FL_ID</th>\n",
       "      <th>RM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6219244</td>\n",
       "      <td>ELECTRICAL SERVICES</td>\n",
       "      <td>TAWANDA LAWYER CHITSIKE</td>\n",
       "      <td>Clo</td>\n",
       "      <td>4/8/2021 0:00</td>\n",
       "      <td>Dear COS, the lights in the male toilet on lev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F03</td>\n",
       "      <td>03</td>\n",
       "      <td>T302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6219892</td>\n",
       "      <td>PLUMBING SERVICES</td>\n",
       "      <td>SALLY CHIK</td>\n",
       "      <td>Rej</td>\n",
       "      <td>4/13/2021 0:00</td>\n",
       "      <td>The zip tap in the library staff room is not w...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>G02</td>\n",
       "      <td>01</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6219909</td>\n",
       "      <td>LIGHTING</td>\n",
       "      <td>ANDREW LONG BAO HUA DUONG</td>\n",
       "      <td>Clo</td>\n",
       "      <td>4/13/2021 0:00</td>\n",
       "      <td>Lights are currently out, previously occurred ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>H04</td>\n",
       "      <td>01</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6219912</td>\n",
       "      <td>BLDG ACCESSCARDS</td>\n",
       "      <td>PAUL CHRISTOPHER BROOKS</td>\n",
       "      <td>Com</td>\n",
       "      <td>4/13/2021 0:00</td>\n",
       "      <td>Can i please get access for Celine\\tBoehm\\t116...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6219913</td>\n",
       "      <td>FIRE SERVICES</td>\n",
       "      <td>JOSH MCALPINE</td>\n",
       "      <td>Clo</td>\n",
       "      <td>4/13/2021 0:00</td>\n",
       "      <td>NON CRITICAL DEFECT\\nThe syphonics pipe collar...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118250</th>\n",
       "      <td>6767491</td>\n",
       "      <td>ELECTRICAL SERVICES</td>\n",
       "      <td>MARK JOSEPH BUSETTO</td>\n",
       "      <td>Com</td>\n",
       "      <td>10/7/2022 0:00</td>\n",
       "      <td>2 electrical techs with site knowledge  to be ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>G12</td>\n",
       "      <td>XS</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118251</th>\n",
       "      <td>6752345</td>\n",
       "      <td>PLUMBING SERVICES</td>\n",
       "      <td>ROBERT LIPSCOMBE</td>\n",
       "      <td>Com</td>\n",
       "      <td>9/28/2022 0:00</td>\n",
       "      <td>All of the filters at the main tank and the co...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>E20G</td>\n",
       "      <td>01</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118252</th>\n",
       "      <td>6752571</td>\n",
       "      <td>PLUMBING SERVICES</td>\n",
       "      <td>JOSIE CLARKE</td>\n",
       "      <td>Com</td>\n",
       "      <td>9/28/2022 0:00</td>\n",
       "      <td>Hot water not working in Mayfarm cottage 2\\n\\n...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>S2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118253</th>\n",
       "      <td>6767679</td>\n",
       "      <td>ROOF AND GUTTER</td>\n",
       "      <td>COLLEEN RITCHARD</td>\n",
       "      <td>Com</td>\n",
       "      <td>10/7/2022 0:00</td>\n",
       "      <td>Please check leaking gutter</td>\n",
       "      <td>13.0</td>\n",
       "      <td>C08DA</td>\n",
       "      <td>RF</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118254</th>\n",
       "      <td>6768521</td>\n",
       "      <td>BLDG ACCESSCARDS</td>\n",
       "      <td>ASHRIKA ACHARYA</td>\n",
       "      <td>Com</td>\n",
       "      <td>10/12/2022 0:00</td>\n",
       "      <td>Could you please provide access to the student...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>G04</td>\n",
       "      <td>05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118255 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          WR_ID            PROB_TYPE                  REQUESTOR STATUS  \\\n",
       "0       6219244  ELECTRICAL SERVICES    TAWANDA LAWYER CHITSIKE    Clo   \n",
       "1       6219892    PLUMBING SERVICES                 SALLY CHIK    Rej   \n",
       "2       6219909             LIGHTING  ANDREW LONG BAO HUA DUONG    Clo   \n",
       "3       6219912     BLDG ACCESSCARDS    PAUL CHRISTOPHER BROOKS    Com   \n",
       "4       6219913        FIRE SERVICES              JOSH MCALPINE    Clo   \n",
       "...         ...                  ...                        ...    ...   \n",
       "118250  6767491  ELECTRICAL SERVICES        MARK JOSEPH BUSETTO    Com   \n",
       "118251  6752345    PLUMBING SERVICES           ROBERT LIPSCOMBE    Com   \n",
       "118252  6752571    PLUMBING SERVICES               JOSIE CLARKE    Com   \n",
       "118253  6767679      ROOF AND GUTTER           COLLEEN RITCHARD    Com   \n",
       "118254  6768521     BLDG ACCESSCARDS            ASHRIKA ACHARYA    Com   \n",
       "\n",
       "         DATE_REQUESTED                                        DESCRIPTION  \\\n",
       "0         4/8/2021 0:00  Dear COS, the lights in the male toilet on lev...   \n",
       "1        4/13/2021 0:00  The zip tap in the library staff room is not w...   \n",
       "2        4/13/2021 0:00  Lights are currently out, previously occurred ...   \n",
       "3        4/13/2021 0:00  Can i please get access for Celine\\tBoehm\\t116...   \n",
       "4        4/13/2021 0:00  NON CRITICAL DEFECT\\nThe syphonics pipe collar...   \n",
       "...                 ...                                                ...   \n",
       "118250   10/7/2022 0:00  2 electrical techs with site knowledge  to be ...   \n",
       "118251   9/28/2022 0:00  All of the filters at the main tank and the co...   \n",
       "118252   9/28/2022 0:00  Hot water not working in Mayfarm cottage 2\\n\\n...   \n",
       "118253   10/7/2022 0:00                        Please check leaking gutter   \n",
       "118254  10/12/2022 0:00  Could you please provide access to the student...   \n",
       "\n",
       "        SITE_ID  BL_ID FL_ID RM_ID  \n",
       "0           1.0    F03    03  T302  \n",
       "1           2.0    G02    01   116  \n",
       "2           2.0    H04    01   124  \n",
       "3           1.0    A28   NaN   NaN  \n",
       "4           1.0    B23   NaN   NaN  \n",
       "...         ...    ...   ...   ...  \n",
       "118250      2.0    G12    XS   NaN  \n",
       "118251     13.0   E20G    01   100  \n",
       "118252     11.0     S2   NaN   NaN  \n",
       "118253     13.0  C08DA    RF   NaN  \n",
       "118254      2.0    G04    05   NaN  \n",
       "\n",
       "[118255 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Archibus WR Data_2021-2022_20221018.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67bd49",
   "metadata": {},
   "source": [
    "for category in doc:\n",
    "    df.groupby['PROB_TYPE']\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646ff525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.punct = list(string.punctuation.replace(\"'\", '’'))\n",
    "        self.token_list = [\n",
    "          '|', '------','____','If you have received this email in error ','Any use, distribution, disclosure, or copying of this email','cheers', 'regards', 'best regards', 'kind regards',\n",
    "          'sent from:', 'from:', 'sent:', 'to:', 'cc:', 'subject:', 'Sender:','date: ',' outlook for', '<https://protect','This short survey will take','how satisfied or dissatisfied are you with our',\n",
    "          'CISCO', '[image]', '.pdf', '[cid:', 'Giving today. Changing tomorrow.','If you have a different query, you will need to submit a new enquiry', \n",
    "          'See the impact<', 'This email plus any attachments to it are confidential. Any unauthori','Read through your handbook<http','find out what you will need to enrol in your new course<https',\n",
    "          'We acknowledge and respect the', 'Please think of our environment','If you are not satisfied with this response, you can re-open the enquiry',\n",
    "          'This e-mail may contain information which is confidential', 'Before commencing your research you will need to enrol, to review the details of your approved',\n",
    "          'This email (including any attachments) may contain confidential','We acknowledge the traditional custodians of the land','This communication may contain information that is proprietary',\n",
    "          'Views expressed in this email are those of the individual sender, and are not necessarily the views of', 'You can find out how to suspend your studies<https',\n",
    "          'This email plus any attachments to it are confidential','for any other general enquiries you are welcome to contact the Student Centre',\n",
    "          '=========', 'Please quote your full name and student number in all correspondence with the University.','Your student card is valid for the duration of your enrolment',\n",
    "          'Please use your student email account if you are a current student for all communication with the University.','Thinking about taking a break from your studies',\n",
    "          'The majority of correspondence issued by the University of Sydney is sent via email. In order to ensure','You can find more information about enrolment<',\n",
    "          'To update this case please click on the link in the email below, or alternatively reply above this lin','After you have successfully enrolled, we will send a confirmation summary to your University student',\n",
    "          'If you receive this email in error', 'It is strictly forbidden to share any part of this message','If you have any questions, call us on 1800 SYD UNI'\n",
    "        ]\n",
    "        self.token_list = [t.lower() for t in self.token_list]\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if isinstance(args[0], list):\n",
    "            [self(arg) for arg in args[0]]\n",
    "        elif isinstance(args[0], pd.DataFrame):\n",
    "            self.replace_DESCRIPTION(args[0])\n",
    "            self.remove_blacklisted_sentences(args[0])\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "    def remove_punctuations(self, text):\n",
    "        for punctuation in self.punct:\n",
    "            text = text.replace(punctuation, ' ')\n",
    "        return text\n",
    "\n",
    "    def str_blacklist(self, string):\n",
    "        str_list = string.split('\\n')\n",
    "        return '\\n'.join([s for s in str_list if not any(t in s.lower() for t in self.token_list) and len(s) != 0])\n",
    "      \n",
    "    def remove_blacklisted_sentences(self, df):\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(self.str_blacklist)\n",
    "\n",
    "    def replace_DESCRIPTION(self, df):\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].astype(str)\n",
    "        \n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(lambda x: x.lower())\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(lambda x: x.replace('\\n', ' '))\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace('http\\S+|www.\\S+', '', regex=True)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(lambda x: x.replace('&gt;', ''))\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(self.remove_punctuations)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].apply(lambda x: str(x).replace(\" s \", \" \"))\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"\\r\", \" \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(' university of sydney',' usyd ', case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(' university ', ' usyd ', case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(' uni ',' usyd ',case=False)  #do not use this line it will mess up unit as usydt\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"sydney student\",\"sydney_student\",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"contact us\", \"contact_us\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"popular questions\",\"popular_questions\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(' center ',' centre ',case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"student centre\", \"student_centre\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"unit of study\",\"unit_of_study\", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" units of study \",\" unit_of_study \",case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" masters \",\" master \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"online enquiry\", \"online_enquiry\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"online inquiry\", \"online_inquiry\", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" re enrol \",\" re_enrol \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" enroll \", \" enrol \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" enrolment \",\" enrol \",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" enroling \",\" enrol \",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" enroled \",\" enrol \",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" enrolled \",\" enrol \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" re enrollment \", \" re_enrol \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" student id \" , \" student_id \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" log into \",\" login \",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" log in \",\" login \",case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\"error message \", \" error_message \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" international students \", \" international_students \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" international student \", \" international_students \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" first year \", \" first_year \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" final year \", \" final_year \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" census date \", \" census_date \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" none of the above \", \" none_of_the_above \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(' units ',' unit ', case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" SID \", \" student_id \", case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" summer school \",\" summer_school \",case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" collecting \",\" collection \",case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" collect \",\" collection \",case=False)\n",
    "\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" studies \",\" study \",case=False)\n",
    "        #new\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" students \", \" student \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" covid\", \" coronavirus \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" corona virus \", \" coronavirus \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" suspension \", \" suspend \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" universitys \", \" usyd \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" begin \", \" start \", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" face to face \", \" face_to_face\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" reenrol\", \" re_enrol\", case=False)\n",
    "        df['DESCRIPTION'] = df['DESCRIPTION'].str.replace(\" australian \", \" australia \", case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "120a0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessor(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a344405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, nlp, stop_words):\n",
    "        self.stop_words = set(stop_words)\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __call__(self, df, key='DESCRIPTION', **kwargs):\n",
    "        self.data_lemmatized(df, key)\n",
    "        \n",
    "    def data_words(self, series):\n",
    "        return list(self.sent_to_words(series.values.tolist()))\n",
    "    \n",
    "    def sent_to_words(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "    def make_bigrams(self, bigram_mod, texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "    \n",
    "    def remove_stopwords(self, texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in self.stop_words] for doc in texts]\n",
    "    \n",
    "    def lemmatization(self, texts, allowed_postags=('NOUN', 'VERB')):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = self.nlp(\" \".join(sent))\n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "    \n",
    "    @singledispatchmethod\n",
    "    def data_lemmatized(self, series, key='DESCRIPTION', **kwargs):\n",
    "        data_words = self.data_words(series)\n",
    "        data_words_nostops = self.remove_stopwords(data_words)\n",
    "        \n",
    "        bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "        \n",
    "        data_words_bigrams = self.make_bigrams(bigram_mod, data_words_nostops)\n",
    "        \n",
    "        return self.lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB'])\n",
    "    \n",
    "    @data_lemmatized.register(list)\n",
    "    def _(self, df_list, key='DESCRIPTION', **kwargs):\n",
    "        return [self.data_lemmatized(df, key, **kwargs) for df in df_list]\n",
    "    \n",
    "    @data_lemmatized.register(pd.DataFrame)\n",
    "    def _(self, df, key='DESCRIPTION', out_key='data_words', **kwargs):\n",
    "        df[out_key] = self.data_lemmatized(df[key], key='DESCRIPTION', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62bc5a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         dear cos  the lights in the male toilet on lev...\n",
       "1         the zip tap in the library staff room is not w...\n",
       "2         lights are currently out  previously occurred ...\n",
       "3         can i please get access for celine\\tboehm\\t116...\n",
       "4         non critical defect the syphonics pipe collar ...\n",
       "                                ...                        \n",
       "118250    2 electrical techs with site knowledge  to be ...\n",
       "118251    all of the filters at the main tank and the co...\n",
       "118252    hot water not working in mayfarm cottage 2  ha...\n",
       "118253                          please check leaking gutter\n",
       "118254    could you please provide access to the student...\n",
       "Name: DESCRIPTION, Length: 118255, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_constructor = Corpus(nlp, stop_words)\n",
    "corpus_constructor(df)\n",
    "\n",
    "text = df['DESCRIPTION']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(text)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "results = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dba04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pd.api.extensions.register_dataframe_accessor('_topic')\n",
    "class TopicFrame:\n",
    "    \n",
    "    def __init__(self, pandas_obj):\n",
    "        self._obj = pandas_obj\n",
    "    \n",
    "    @property\n",
    "    def hierarchy(self):\n",
    "        return ['u_category', 'u_wrap_up_code', 'contact_type', 'date_range']\n",
    "    \n",
    "    def keys_from_column(self, column):\n",
    "        return self._obj[column].unique()\n",
    "    \n",
    "    def keys_from_columns(self, columns):\n",
    "        return itertools.product(*[self.keys_from_column(col) for col in columns])\n",
    "    \n",
    "    def dataframe_from_keys(self, columns, keys):\n",
    "        return self._obj.query(' and '.join([f'{col} == \"{key}\"' for col, key in zip(columns, keys)]))\n",
    "    \n",
    "    def iterate(self, columns=None, return_keys=False):\n",
    "        columns = self.hierarchy if columns is None else columns\n",
    "        for keys in self.keys_from_columns(columns):\n",
    "            df = self.dataframe_from_keys(columns, keys)\n",
    "            if df.shape[0] != 0:\n",
    "                if return_keys:\n",
    "                    yield df, {col: key for col, key in zip(columns, keys)}\n",
    "                else:\n",
    "                    yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49327055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(dictionary, texts):\n",
    "    return [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "def get_dictionary(texts):\n",
    "    return gensim.corpora.Dictionary(texts) \n",
    "\n",
    "\n",
    "def get_optimised_lda_model(dictionary, corpus, texts, limit=14):\n",
    "    try:\n",
    "        top_model = LdaModel(corpus=corpus, num_topics=1, id2word=dictionary)\n",
    "    except ValueError:\n",
    "        dictionary = get_dictionary([['Empty']])\n",
    "        top_model = LdaModel(corpus=corpus, num_topics=1, id2word=dictionary)\n",
    "\n",
    "    top_coherence = 0\n",
    "\n",
    "    limit = min(limit, np.unique(texts).shape[0])\n",
    "    for num_topics in range(2, limit):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        coherence = CoherenceModel(model=top_model, texts=texts, dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "\n",
    "        if np.isnan(coherence):\n",
    "            return top_model\n",
    "        elif coherence > top_coherence:\n",
    "            top_model, top_coherence = model, coherence\n",
    "\n",
    "    return top_model\n",
    "\n",
    "\n",
    "class TopicConstructor:\n",
    "    \n",
    "    def __init__(self, df, data_key='data_words'):\n",
    "        self.df = df\n",
    "        self.df.reset_index(drop=True, inplace=True)\n",
    "        self.data_key = data_key\n",
    "    \n",
    "    def create_topics(self, iterate_columns=None, limit=14):\n",
    "        timer = time.time()\n",
    "        topic_dict_list = []\n",
    "        cont_dict_list = []\n",
    "        for idx, (df, key_dict) in enumerate(self.df._topic.iterate(iterate_columns, return_keys=True)):\n",
    "            print(f'{idx} - {datetime.timedelta(seconds=(time.time() - timer))}')\n",
    "            dictionary = get_dictionary(df[self.data_key])\n",
    "            corpus = get_corpus(dictionary, df[self.data_key])\n",
    "            \n",
    "            lda_model = get_optimised_lda_model(dictionary, corpus, df[self.data_key], limit=limit)\n",
    "            ticket_counter, cont_dicts = self.get_dominant_topic(df, corpus, lda_model, offset=len(topic_dict_list))\n",
    "            cont_dict_list += cont_dicts\n",
    "            \n",
    "            for topic_num in range(lda_model.num_topics):\n",
    "                topic_dict_list.append(dict(topic_num=topic_num,\n",
    "                                            topic_keywords=self.keywords_from_topic(lda_model, topic_num),\n",
    "                                            topic_tickets=ticket_counter[topic_num],\n",
    "                                            oldest_timestamp=df['sys_updated_on'].min(),\n",
    "                                            newest_timestamp=df['sys_updated_on'].max(),\n",
    "                                            **key_dict))\n",
    "                \n",
    "        return pd.DataFrame(topic_dict_list), pd.DataFrame(cont_dict_list)\n",
    "            \n",
    "    def get_dominant_topic(self, df, corpus, lda_model, offset=0):\n",
    "        df_dict = df.to_dict(orient='records')\n",
    "        ticket_counter = Counter()\n",
    "        topic_contribution = []\n",
    "        cont_dict_list = []\n",
    "        for idx, row in enumerate(lda_model[corpus]):\n",
    "            topic_num, contribution = max(row, key=lambda x: x[1])\n",
    "            ticket_counter.update([topic_num])\n",
    "            self.df.loc[df.iloc[idx].name, 'topic_num'] = topic_num\n",
    "            self.df.loc[df.iloc[idx].name, 'unique_topic_num'] = offset + topic_num\n",
    "            self.df.loc[df.iloc[idx].name, 'contribution'] = contribution\n",
    "            self.df.loc[df.iloc[idx].name, 'topic_keywords'] = self.keywords_from_topic(lda_model, topic_num)\n",
    "            for num, cont in row:\n",
    "                cont_dict_list.append({**df_dict[idx], 'topic_num': num, 'unique_topic_num': offset + num, 'contribution': cont})\n",
    "        \n",
    "        return ticket_counter, cont_dict_list\n",
    "            \n",
    "            \n",
    "    def keywords_from_topic(self, lda_model, topic_num):\n",
    "        return ', '.join([w[0] for w in lda_model.show_topic(topic_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2761a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = topic_out_df._topic.hierarchy\n",
    "cols = ['u_category', 'contact_type']\n",
    "topic_out_df, cont_out_df = topic_constructor.create_topics(cols, limit=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ac3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_cols = cols + ['topic_num']\n",
    "\n",
    "keyword_list = []\n",
    "for idx, row in topic_out_df.iterrows():\n",
    "    data_words = df._topic.dataframe_from_keys(_cols, [row[col] for col in _cols])['data_words']\n",
    "    word_counter = Counter([word for r in data_words for word in r])\n",
    "    \n",
    "    for keyword in row['topic_keywords'].split(', '):\n",
    "        keyword_list.append([keyword, idx, row['topic_num'],  word_counter[keyword]])\n",
    "        \n",
    "keyword_df = pd.DataFrame(keyword_list, columns=['keywords', 'unique_topic', 'topic_num', 'topic_keyword_count'])\n",
    "\n",
    "# Remove keywords if < 2 characters and remove all keywords if < 2 keywords present\n",
    "topic_out_df['topic_keywords'] = topic_out_df['topic_keywords'].apply(lambda x: [w for w in x.split(', ') if len(w) > 2]).apply(lambda x: '' if len(x) < 2 else ', '.join(x))\n",
    "\n",
    "# topic_out_df.to_csv('topics.csv')\n",
    "# keyword_df.to_csv('keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e194502",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cffaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
